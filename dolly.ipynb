{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvIZTkbNnfcnIE0ZI8M9aA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlnzlk/gpt/blob/master/dolly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTrdFXVCZCgl",
        "outputId": "d01abf15-8de7-4b18-cb1f-6fb5b9345de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "#colab GPU 정보 확인\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 필요한 라이브러리 설치\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip uninstall transformers\n",
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RyHO1Bmbynm",
        "outputId": "7e1ca42a-5a09-484a-dedb-78655bcac20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 기본으로 깔려있는 pytorch 버전이 맞지않아 삭제하고 재설치\n",
        "!pip uninstall -y torch\n",
        "!pip uninstall -y torchvision\n",
        "!pip -q install torch==1.13.1\n",
        "!pip -q install torchvision==0.14.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU_dWbTPcCYY",
        "outputId": "216a2cdd-9fdc-426a-c88a-7583c68d9f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.1+cu118\n",
            "Uninstalling torch-2.0.1+cu118:\n",
            "  Successfully uninstalled torch-2.0.1+cu118\n",
            "Found existing installation: torchvision 0.15.2+cu118\n",
            "Uninstalling torchvision-0.15.2+cu118:\n",
            "  Successfully uninstalled torchvision-0.15.2+cu118\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.12 requires torchvision>=0.8.2, which is not installed.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "알파카 데이터셋은 직접 만든 학습이 되는지 테스트를 위해 사용한 것이고,\n",
        "직접 데이터를 만들어서 사용할 때 는 생략"
      ],
      "metadata": {
        "id": "Bj-xMywxcMcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 알파카 데이터셋 로드\n",
        "!git clone https://github.com/gururise/AlpacaDataCleaned.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBO9AqCtcOEi",
        "outputId": "7c7cad5a-cc0d-425a-e005-1659f2146dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AlpacaDataCleaned'...\n",
            "remote: Enumerating objects: 747, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 747 (delta 64), reused 94 (delta 53), pack-reused 623\u001b[K\n",
            "Receiving objects: 100% (747/747), 76.51 MiB | 17.24 MiB/s, done.\n",
            "Resolving deltas: 100% (411/411), done.\n",
            "Updating files: 100% (69/69), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 구글 클라우드 서비스 연결. Colab에서 세션이 만료되면 사라지는 파일을 관리하기 위해 사용. 만약 직접 파일을 로컬 컴퓨터에서 업로드한다면 구글 클라우드 관련 내용은 전부 생략 가능\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "tFbA3nJkcgAO",
        "outputId": "fad9df42-7a75-4c83-9503-2a0a5133c52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0cfa9532912a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title 구글 클라우드 서비스 연결. Colab에서 세션이 만료되면 사라지는 파일을 관리하기 위해 사용. 만약 직접 파일을 로컬 컴퓨터에서 업로드한다면 구글 클라우드 관련 내용은 전부 생략 가능\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gcloud init'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output, project_id)\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CredentialType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_auth_ephem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       _message.blocking_request(\n\u001b[0m\u001b[1;32m    282\u001b[0m           \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m           \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auth_user_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 구글 클라우드 서비스안에 있는 버킷의 경로, 버킷은 디렉토리 같은 것\n",
        "path_to_cloud_bucket = 'gs://gptneo-testdata230410/' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "yAr9r9-QckaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 구글 클라우스 서비스에서 데이터셋을 다운로드\n",
        "from google.cloud import storage\n",
        "\n",
        "bucket_name = 'gptneo-testdata230410'    # 서비스 계정 생성한 bucket 이름 입력\n",
        "source_blob_name = 'instructions1000.jsonl'    # GCP에 저장되어 있는 파일 명\n",
        "destination_file_name = './data/MyData.jsonl'    # 다운받을 파일을 저장할 경로(\"local/path/to/file\")\n",
        "\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(source_blob_name)\n",
        "\n",
        "blob.download_to_filename(destination_file_name)"
      ],
      "metadata": {
        "id": "3J4_UWqtcmCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 같은 데이터 천개로 구성된 데이터셋을 만들기 위해 사용했던 코드. 데이터셋이 있으면 생략 가능\n",
        "import json\n",
        "\n",
        "data_created = []\n",
        "output_filename_created = \"./data/MyData.jsonl\"\n",
        "\n",
        "for i in range (1,1000):\n",
        "  data_created.append({'instruction':'\"Action: Open Time: 2023-05-15 13:00 File: Microsoft Edge Day: Monday\"\\nthis is a log from what user doing\\n.Give me pseudo code for the job that user was doing.','context':'','response': '# Open Microsoft Edge on Monday at 13:00\\n\\n# Check if it is Monday.\\nif (today == \"Monday\"):\\n\\n  # Open Microsoft Edge.\\n  open \"Microsoft Edge\"'})\n",
        "\n",
        "with open(output_filename_created, 'w') as f:\n",
        "    for data in data_created:\n",
        "        f.write(json.dumps(data) + '\\n')"
      ],
      "metadata": {
        "id": "ggyMisoLcnlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 토크나이저 로드\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\") #사용할 코트나이저 이름 입력\n",
        "\n",
        "\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "data = load_dataset(\"json\", data_files=\"./data/MyData.jsonl\") #사용할 데이터셋\n",
        "\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    # taken from https://github.com/tloen/alpaca-lora\n",
        "    if data_point[\"instruction\"]:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### context:\n",
        "{data_point[\"context\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"response\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"response\"]}\"\"\"\n",
        "\n",
        "\n",
        "data = data.map(lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))})\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "WrEeSYBwcoih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Finetuning을 위한 라이브러리들을 import해주는 코드 셀\n",
        "import os\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "rxj4C-DPcsAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 학습 설정\n",
        "# Settings for A100 - For 3090\n",
        "MICRO_BATCH_SIZE = 4  # change to 4 for 3090\n",
        "#Batch Size는 한 번 가중치를 업데이트하기 위해 사용할 데이터의 개수. 64로 설정되어있으면 한 번 가중치를 업데이트하기 위해 64개의 데이터셋을 사용하는것\n",
        "#데이터의 개수가 1000개이고 배치사이즈가 64이면 가중치 업데이트가 약 20번 일어남\n",
        "BATCH_SIZE = 64\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 2  # paper uses 3 전체 데이터셋을 학습할 횟수. 1000개의 데이터셋을 에포크 값 2로 학습하면 2000개의 데이터를 학습하는 것이다.\n",
        "LEARNING_RATE = 2e-5  #학습할 때 가중치값의 변동률. 값이 클수록 학습할 때 가중치 값이 크게 변한다.\n",
        "CUTOFF_LEN = 256\n",
        "LORA_R = 4\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05"
      ],
      "metadata": {
        "id": "Gmo4JYTpct_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 토크나이저와 베이스 모델 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\",\n",
        "                                          add_eos_token=True,\n",
        "                                          )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8b\",\n",
        "                                  load_in_8bit=True,\n",
        "                                  device_map=\"auto\",\n",
        "                                  )\n",
        "tokenizer.pad_token_id = 0\n",
        "\n",
        "model = prepare_model_for_int8_training(model, use_gradient_checkpointing=True)"
      ],
      "metadata": {
        "id": "gzjcHSqHcvu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 학습 설정2\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "\n"
      ],
      "metadata": {
        "id": "Ry9sRaqEcxhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 데이터셋 로드\n",
        "data = load_dataset(\"json\", data_files=\"./data/MyData.jsonl\")\n",
        "data = data.shuffle().map(\n",
        "    lambda data_point: tokenizer(\n",
        "        generate_prompt(data_point),\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "pE3mP6nLcyJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "oZLqO8z8cz8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 학습 시작\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"lora-dolly\", #저장할 폴더명\n",
        "        save_total_limit=3,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "model.save_pretrained(\"dolly-3b-1x1000\") #저장할 모델명"
      ],
      "metadata": {
        "id": "iputvJR8c57B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 모델 실행을 위한 라이브러리 import, 토크나이저와 모델 import\n",
        "import os\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "tokenizerg = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\",\n",
        "                                          add_eos_token=True,\n",
        "                                          )\n",
        "tokenizerg.pad_token = tokenizerg.eos_token\n",
        "tokenizerg.pad_token_id = tokenizerg.eos_token_id\n",
        "\n",
        "modelg = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8b\",\n",
        "                                  load_in_8bit=True,\n",
        "                                  device_map=\"auto\",\n",
        "                                  )\n",
        "\n",
        "\n",
        "modelg = PeftModel.from_pretrained(modelg, \"dolly-3b-1x1000\")"
      ],
      "metadata": {
        "id": "vFCd0u_fc_Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 모델 실행 코드 정의\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "def dolly_generate(text):\n",
        "    inputs = tokenizerg(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "\n",
        "    print(\"Generating...\")\n",
        "    generation_output = modelg.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=128,\n",
        "        pad_token_id = 0,\n",
        "        eos_token_id = 50256\n",
        "    )\n",
        "\n",
        "    for s in generation_output.sequences:\n",
        "        print(tokenizerg.decode(s))"
      ],
      "metadata": {
        "id": "gLT7xmmcdBCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PROMPT에 입력값 설정 후 실행, instruction에 질문, 지시사항 등이 들어가고 input에 추가 정보가 들어감. 테스트할 당시에는 추가정보로 로그 정보를 입력\n",
        "PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "'Action: Open, Time: 2023-05-15 13:00, File: Microsoft Edge, Day: Monday'\n",
        "This is a log from what user doing.\n",
        "Give me pseudo code for the job that user was doing.\n",
        "### input:\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "dolly_generate(PROMPT)"
      ],
      "metadata": {
        "id": "yun-csX9dCev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TtYk9A_rdEMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}